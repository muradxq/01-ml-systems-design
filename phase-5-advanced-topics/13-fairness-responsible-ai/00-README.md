# âš–ï¸ Fairness & Responsible AI

## Overview

Fairness and responsible AI are **critical differentiators** in ML Systems Design interviews at Meta, Google, and other top tech companies. Interviewers explicitly assess whether candidates think proactively about bias, fairness, and ethical deploymentâ€”not as an afterthought, but as integral parts of system design. Systems that perpetuate discrimination, lack transparency, or cause harm to protected groups fail both ethically and in production.

This section covers the complete fairness lifecycle: detecting bias, implementing fairness constraints, auditing models, and deploying responsibly. You'll learn to anticipate fairness failures before they occur and design systems that uphold ethical standards while maintaining model utility.

---

## ğŸ¯ Learning Objectives

After completing this section, you should understand:

- **Bias taxonomy**: Historical, representation, measurement, aggregation, evaluation, deployment, selection, and feedback loop bias
- **Fairness metrics**: Demographic parity, equalized odds, calibration, individual fairness, counterfactual fairness
- **Intervention strategies**: Pre-processing, in-processing, and post-processing approaches
- **Model auditing**: Model cards, datasheets for datasets, explainability methods (SHAP, LIME)
- **Responsible deployment**: Feedback loops, human-in-the-loop, monitoring, appeals processes

---

## ğŸ”¬ Why Fairness Is a Pass/Fail Differentiator

At Meta, Google, and similar companies, ML Systems Design interviews explicitly probe fairness and responsible AI. Interviewers are trained to notice:

1. **Proactive vs. reactive**: Do you bring up fairness unprompted, or only when asked?
2. **Depth**: Can you discuss bias taxonomy, fairness metrics, and trade-offs?
3. **Practicality**: Do you mention specific techniques (sliced eval, model cards, human-in-the-loop)?
4. **Real-world awareness**: Do you reference known failures (COMPAS, Amazon hiring, facial recognition)?

Candidates who integrate fairness throughout their system designâ€”from data to deploymentâ€”signal the kind of responsible engineering these companies expect. Those who treat it as an afterthought often receive negative feedback, even when other aspects of the design are strong.

---

## ğŸ“Š The Fairness Lifecycle

Fairness considerations span the entire ML pipeline. Bias can enter at any stage and propagate or amplify downstream.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         THE FAIRNESS LIFECYCLE                                                â”‚
â”‚                                                                                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚   â”‚     DATA     â”‚â”€â”€â”€â–¶â”‚   TRAINING   â”‚â”€â”€â”€â–¶â”‚  EVALUATION  â”‚â”€â”€â”€â–¶â”‚  DEPLOYMENT  â”‚â”€â”€â”€â–¶â”‚ MONITORâ”‚â”‚
â”‚   â”‚              â”‚    â”‚              â”‚    â”‚              â”‚    â”‚              â”‚    â”‚ -ING    â”‚â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚          â”‚                    â”‚                   â”‚                   â”‚                  â”‚    â”‚
â”‚          â–¼                    â–¼                   â–¼                   â–¼                  â–¼    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚   â”‚ BIAS RISK:   â”‚    â”‚ BIAS RISK:   â”‚    â”‚ BIAS RISK:   â”‚    â”‚ BIAS RISK:   â”‚    â”‚ BIAS   â”‚â”‚
â”‚   â”‚ â€¢ Historical â”‚    â”‚ â€¢ Aggregationâ”‚    â”‚ â€¢ Evaluation â”‚    â”‚ â€¢ Deployment â”‚    â”‚ RISK:  â”‚â”‚
â”‚   â”‚ â€¢ Represent. â”‚    â”‚ â€¢ Proxy      â”‚    â”‚   bias       â”‚    â”‚   bias       â”‚    â”‚ â€¢ Driftâ”‚
â”‚   â”‚ â€¢ Measurementâ”‚    â”‚   learning   â”‚    â”‚ â€¢ Incomplete â”‚    â”‚ â€¢ Access     â”‚    â”‚ â€¢ Feed-â”‚
â”‚   â”‚ â€¢ Selection  â”‚    â”‚ â€¢ Regulariz. â”‚    â”‚   slicing    â”‚    â”‚   disparity  â”‚    â”‚   back â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                                               â”‚
â”‚   MITIGATION:              MITIGATION:            MITIGATION:            MITIGATION:         â”‚
â”‚   â€¢ Resampling             â€¢ Adversarial           â€¢ Sliced evaluation    â€¢ Threshold         â”‚
â”‚   â€¢ Reweighting            â€¢ debiasing             â€¢ Fairness metrics     â€¢ adjustment       â”‚
â”‚   â€¢ Data augmentation      â€¢ Fairness             â€¢ Intersectional       â€¢ Human review      â”‚
â”‚   â€¢ Datasheets              regularization         analysis               â€¢ Appeals          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Stage-by-Stage Fairness Considerations

| Stage | Key Questions | Common Failures |
|-------|---------------|-----------------|
| **Data** | Who is represented? How was data collected? What proxies exist? | Underrepresentation of minorities; biased labels; proxy discrimination |
| **Training** | Does the objective penalize unfair behavior? Are there fairness constraints? | Model learns historical discrimination; proxy features dominate |
| **Evaluation** | Are we evaluating across subgroups? Using appropriate fairness metrics? | Aggregate metrics hide disparity; single-group evaluation |
| **Deployment** | Who has access? Are thresholds calibrated per group? | Disparate access; thresholding amplifies gaps |
| **Monitoring** | Do we track fairness metrics over time? Are there alerts? | Drift goes undetected; feedback loops amplify bias |

---

## ğŸ“š Table of Contents

| # | Topic | Description |
|---|-------|--------------|
| 1 | [Bias Detection](./01-bias-detection.md) | Types of bias, fairness metrics (demographic parity, equalized odds, calibration), sliced evaluation, intersectional analysis, Python implementations |
| 2 | [Fairness Constraints](./02-fairness-constraints.md) | Pre-processing (resampling, reweighting), in-processing (adversarial debiasing, constraints), post-processing (thresholds), fairness-accuracy trade-offs |
| 3 | [Model Auditing](./03-model-auditing.md) | Model cards, datasheets for datasets, explainability (SHAP, LIME, IG), audit processes, regulatory frameworks |
| 4 | [Responsible Deployment](./04-responsible-deployment.md) | Feedback loops, content policy, human-in-the-loop, appeals, fairness monitoring, red teaming, case studies |

---

## ğŸ”‘ Key Concepts

### Protected Attributes

Attributes that should not be used for discriminatory decisions (often legally protected):

| Attribute | Examples | Notes |
|-----------|----------|-------|
| **Race** | Self-reported, inferred | Never use for decisions; proxy risk |
| **Gender** | Binary, non-binary | Legal protections vary by jurisdiction |
| **Age** | Continuous, buckets | Proxy for experience in hiring |
| **Religion** | Categorical | Rarely in data; inferred proxies |
| **Disability** | Binary, categorical | Accessibility implications |
| **National origin** | Country, region | Immigration, language proxies |

**Critical insight**: Removing protected attributes from features **does not prevent discrimination**. Correlated proxies (zip code â†’ race, name â†’ gender) allow models to replicate bias. You must explicitly test and constrain fairness.

---

### Disparate Impact

A policy or model has **disparate impact** when it adversely affects a protected group compared to a reference group, even without intent.

```
                    Positive outcome rate (e.g., loan approval)
                    
Reference Group (e.g., Group A)     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  80%
Protected Group (e.g., Group B)     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          60%

                    Disparate Impact Ratio = 60% / 80% = 0.75
                    
                    Rule of thumb: < 0.8 may violate EEOC "80% rule"
```

**Formula**: Disparate impact ratio = P(Å·=1 | protected) / P(Å·=1 | reference)

---

### Fairness-Accuracy Trade-off

There is **no free lunch**: enforcing fairness constraints often reduces overall accuracy. Different definitions of fairness can also conflict (e.g., demographic parity vs. equalized odds).

```
                    Accuracy
                        â–²
                        â”‚     Pareto frontier
                        â”‚        â—
                        â”‚       /
                        â”‚      /  â— Unconstrained
                        â”‚     /
                        â”‚    â—  Fairness-constrained
                        â”‚   /
                        â”‚  /
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Fairness (e.g., parity gap)
```

**Interview tip**: Acknowledge the trade-off. Propose measuring the Pareto frontier, setting fairness thresholds as requirements, and selecting the most accurate model that meets them.

---

## ğŸ›ï¸ Architecture Patterns for Fairness

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FAIRNESS-AWARE ML SYSTEM ARCHITECTURE                          â”‚
â”‚                                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚   Data      â”‚    â”‚   Training  â”‚    â”‚  Inference  â”‚    â”‚  Monitoring â”‚      â”‚
â”‚   â”‚   Pipeline  â”‚â”€â”€â”€â–¶â”‚   + Fairnessâ”‚â”€â”€â”€â–¶â”‚  + Post-    â”‚â”€â”€â”€â–¶â”‚  + Fairness â”‚      â”‚
â”‚   â”‚   + Slice   â”‚    â”‚   Constraintâ”‚    â”‚  Processing â”‚    â”‚  Dashboards  â”‚      â”‚
â”‚   â”‚   Metadata  â”‚    â”‚   or Re-    â”‚    â”‚  (optional) â”‚    â”‚  + Alerts    â”‚      â”‚
â”‚   â”‚             â”‚    â”‚   weighting â”‚    â”‚             â”‚    â”‚             â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚          â”‚                   â”‚                   â”‚                   â”‚            â”‚
â”‚          â–¼                   â–¼                   â–¼                   â–¼            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚  Datasheet  â”‚  Model Card  â”‚  Human Review  â”‚  Appeals  â”‚  Audit Log   â”‚    â”‚
â”‚   â”‚  for Data   â”‚  + Fairness  â”‚  Queue (HITL)  â”‚  API      â”‚  (decisions) â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key components**:
- **Slice metadata**: Store protected-group labels (with consent) for evaluation and monitoring
- **Fairness constraints**: Applied at training (in-processing) or inference (post-processing)
- **Human review**: Queue for low-confidence or high-stakes decisions
- **Fairness dashboards**: Real-time parity ratios, TPR/FPR by slice
- **Audit log**: Record decisions and outcomes for external audit and appeals

---

## ğŸš¨ Why This Matters for Interviews

At Meta and Google, fairness questions often appear as:

- **"How would you ensure this hiring model doesn't discriminate?"**
- **"The recommendation system is creating echo chambers. How do you address it?"**
- **"How would you audit this credit model for fairness before launch?"**
- **"We've received complaints about biased outcomes. How do you investigate?"**

**Winning strategy**: Don't wait for the interviewer to ask. When designing any ML system, proactively mention:

1. **Protected groups**: "We'll need to evaluate performance across demographic slices."
2. **Bias sources**: "Historical data may reflect past discriminationâ€”we'll need to handle that."
3. **Metrics**: "We'll track demographic parity and equalized odds in addition to AUC."
4. **Monitoring**: "We'll set up fairness dashboards and alerts for drift."
5. **Human oversight**: "For high-stakes decisions, we'll have human-in-the-loop review."

---

## ğŸ“‹ Fairness Checklist (Pre-Interview)

- [ ] Can define at least 3 types of bias (historical, representation, measurement)
- [ ] Can write formulas for demographic parity, equalized odds, calibration
- [ ] Can explain why removing sensitive features alone doesn't work
- [ ] Can name pre-processing, in-processing, and post-processing approaches
- [ ] Can describe model cards and what they contain
- [ ] Can explain SHAP vs. LIME and when to use each
- [ ] Can discuss feedback loops in recommendation systems
- [ ] Can name 2â€“3 real-world fairness failures (Amazon hiring, COMPAS, facial recognition)

---

## ğŸ“… Suggested Study Plan

| Day | Focus | Activities |
|-----|-------|------------|
| **1** | Bias taxonomy & metrics | Read 01-bias-detection; implement fairness metrics in Python; memorize formulas |
| **2** | Fairness constraints | Read 02-fairness-constraints; trace through adversarial debiasing; practice "when to use which" |
| **3** | Auditing & explainability | Read 03-model-auditing; run SHAP on a toy model; sketch a model card |
| **4** | Deployment & case studies | Read 04-responsible-deployment; review case studies; design HITL + appeals flow |
| **5** | Integration & practice | Do 2â€“3 full system design mocks; integrate fairness into each; practice sample Q&A |

**Tip**: Prioritize the topics that align with your target company (e.g., Meta/Google emphasize rec systems, feedback loops, and content policy). For lending or healthcare, focus on calibration and regulatory requirements.

---

## ğŸš€ Next Steps

1. **[Bias Detection](./01-bias-detection.md)** â€“ Learn to identify and quantify bias in your models.
2. **[Fairness Constraints](./02-fairness-constraints.md)** â€“ Implement interventions at each pipeline stage.
3. **[Model Auditing](./03-model-auditing.md)** â€“ Document and explain models for accountability.
4. **[Responsible Deployment](./04-responsible-deployment.md)** â€“ Deploy safely with monitoring and human oversight.

---

## ğŸ’¬ Sample Interview Q&A

**Q: "How would you build a resume screening model that doesn't discriminate?"**

**A**: "First, I'd acknowledge the risk: historical hiring data reflects past bias. I'd use several strategies: (1) Pre-processingâ€”reweight or oversample underrepresented groups to balance training data. (2) In-processingâ€”adversarial debiasing so the model can't predict protected attributes from its representations, or add fairness regularization. (3) Evaluationâ€”sliced evaluation by gender, race, and intersectionally; track demographic parity and equalized odds, not just aggregate AUC. (4) Post-processingâ€”if needed, threshold adjustment per group to meet parity. (5) Remove explicit sensitive features, but test anywayâ€”proxies like name, school, zip code can encode them. (6) Human review for final decisions. (7) Model card documenting limitations. Amazon's hiring tool failed because it learned proxies; we'd explicitly guard against that."

**Q: "What's the difference between demographic parity and equalized odds?"**

**A**: "Demographic parity says the positive prediction rate should be equal across groupsâ€”P(Å·=1|A=a) = P(Å·=1|A=b). Equalized odds says that among those who should get the positive outcome (Y=1), we should have equal TPR; among those who shouldn't (Y=0), equal FPR. Equalized odds conditions on the true outcome. They can conflict when base rates differâ€”e.g., if one group has a higher rate of qualified candidates, demographic parity might require rejecting qualified people from that group or accepting unqualified from the other. For hiring, we often care about equalized odds. For lending, regulatory scrutiny often focuses on demographic parity (disparate impact)."

**Q: "How do you handle fairness when you don't have demographic data?"**

**A**: "Several options: (1) Optional disclosureâ€”ask users to self-report for evaluation and monitoring; store separately with consent. (2) Proxy analysisâ€”use zip code, name, or other proxies to approximate demographic slices for evaluation; acknowledge the imperfection. (3) Stratified samplingâ€”ensure evaluation set is diverse by geography, institution, or other available factors. (4) External auditingâ€”partner with auditors who can access demographic data under strict controls. (5) Outcome-based monitoringâ€”track outcomes by geography, product segment, or other available dimensions. The goal is to surface disparity even without perfect demographic labels."

**Q: "The recommendation system is creating echo chambers. How do you address it?"**

**A**: "This is a feedback loop problem: we show popular content â†’ users click it â†’ we train on those clicks â†’ we show more of it. To break this: (1) Diversity injectionâ€”explicitly surface content from underserved creators or categories, even if engagement is lower. (2) Explorationâ€”use bandit algorithms or Îµ-greedy to show diverse options; don't purely exploit. (3) Debiased loggingâ€”correct for position and selection bias when training on implicit feedback. (4) Objectivesâ€”consider adding diversity or serendipity to the ranking objective, not just engagement. (5) Monitoringâ€”track diversity metrics (e.g., category entropy, creator concentration) and alert on homogeneity. (6) Holdout evaluationâ€”evaluate on data from random exploration, not model-influenced clicks."

**Q: "We've received complaints about biased outcomes. How do you investigate?"**

**A**: "I'd run a structured investigation: (1) Reproduceâ€”get examples of complained cases; can we reproduce the decision? (2) Sliced analysisâ€”compute fairness metrics (parity, TPR/FPR) by relevant slices; which group is affected? (3) Explainabilityâ€”use SHAP or LIME on complained cases; which features drove the decision? Are proxies for protected attributes prominent? (4) Data auditâ€”check training data composition; any representation or labeling bias? (5) Deployment auditâ€”is the model deployed consistently? Any access or threshold disparity? (6) Appeal reviewâ€”what do human reviewers say about appealed cases? Document findings, implement fixes, update model card, and communicate to stakeholders."

---

## ğŸ“ Fairness Metrics Quick Reference

| Metric | Formula | Use Case |
|--------|---------|----------|
| **Demographic Parity** | P(Å·=1\|A=a) = P(Å·=1\|A=b) | Equal selection rates (hiring, lending) |
| **80% Rule** | min(P(Å·=1\|A)) / max(P(Å·=1\|A)) â‰¥ 0.8 | EEOC disparate impact screening |
| **Equalized Odds** | TPR and FPR equal across groups | Fair error rates (criminal justice, moderation) |
| **Calibration** | P(Y=1\|Å·=s, A=a) = P(Y=1\|Å·=s, A=b) | Risk scores must mean same thing (healthcare) |
| **Individual Fairness** | Similar individuals â†’ similar predictions | Ranking, matching |

---

## ğŸ—£ï¸ Sample Interview Framework: Fairness in System Design

When given any ML system design problem, integrate fairness as follows:

### 1. Clarify Stakeholders and Protected Groups
- "Who are the users? Are there protected groups we need to consider?"
- "What are the potential harms if the system is wrong for a subgroup?"

### 2. Data Phase
- "How was the training data collected? Could it reflect historical bias?"
- "What's the demographic composition? Any underrepresentation?"
- "Are there proxy features for protected attributes we need to be careful about?"

### 3. Model Phase
- "We'll use sliced evaluationâ€”metrics per subgroupâ€”not just aggregate."
- "We'll consider fairness constraints: pre-processing (reweighting), in-processing (adversarial debiasing), or post-processing (threshold adjustment)."
- "We'll document the fairness-accuracy trade-off and get stakeholder alignment."

### 4. Deployment Phase
- "We'll create a model card documenting limitations and evaluation."
- "For high-stakes decisions, we'll have human-in-the-loop."
- "We'll provide an appeals process for users who believe they were wrongly affected."

### 5. Monitoring Phase
- "We'll track fairness metrics in productionâ€”parity ratio, TPR/FPR by slice."
- "We'll alert on drift and set up periodic audits."
- "We'll watch for feedback loops that could amplify bias."

---

## ğŸ“– Key Papers and Resources

| Resource | Focus |
|----------|--------|
| **"Model Cards for Model Reporting"** (Mitchell et al., 2019) | Model documentation |
| **"Datasheets for Datasets"** (Gebru et al., 2018) | Dataset documentation |
| **"A Unified Approach to Interpreting Model Predictions"** (SHAP, Lundberg & Lee, 2017) | Explainability |
| **"Inherent Trade-Offs in the Fair Determination of Risk Scores"** (Chouldechova, 2017) | Calibration vs. equalized odds |
| **ProPublica COMPAS analysis** (2016) | Real-world fairness failure |
| **NIST FRVT** (ongoing) | Facial recognition benchmark by demographic |
| **EU AI Act** (2024) | Regulatory framework |

---

## âš ï¸ Common Interview Mistakes to Avoid

| Mistake | Better Approach |
|---------|-----------------|
| "We'll just remove race/gender from the model" | "We'll remove explicit sensitive features but test for disparate impactâ€”proxies can still cause discrimination. We'll use fairness constraints." |
| "Our model is fair because accuracy is high" | "We'll evaluate across demographic slices. Aggregate accuracy can hide significant disparity." |
| "Fairness is a post-launch concern" | "We'll design for fairness from the startâ€”data collection, evaluation, deployment, and monitoring." |
| "One metric (e.g., demographic parity) is always right" | "Different metrics capture different principles. We'll choose based on the use case and document the trade-offs." |
| "We don't have demographic data" | "We'll need to consider how to get representative evaluation data. Options: optional disclosure, proxy analysis, stratified sampling." |

---

---

## ğŸŒ Fairness Considerations by Domain

| Domain | Key Fairness Concerns | Typical Metrics | Intervention Priorities |
|--------|----------------------|-----------------|-------------------------|
| **Hiring** | Historical bias, name/school proxies | Demographic parity, equalized odds | Pre-processing, in-processing, human review |
| **Lending** | Redlining, thin-file bias | Calibration, disparate impact ratio | Post-processing thresholds, explainability |
| **Healthcare** | Access disparity, outcome gaps | Calibration, equalized odds | Representation in data, sliced evaluation |
| **Criminal justice** | Arrest bias, over-policing | Equalized odds (FPR/FNR) | Careful metric choice, human oversight |
| **Content moderation** | Over-moderation of minorities | TPR/FPR by community | Sliced evaluation, appeal process |
| **Recommendations** | Filter bubbles, under-exposure | Diversity, individual fairness | Exploration, diversity injection |
| **Facial recognition** | Race/gender accuracy gaps | Accuracy by demographic | Diverse training data, fairness testing |
| **Ads** | Discriminatory delivery | Delivery rate by group | Audit ad delivery, policy enforcement |

---

## ğŸ“Š Bias Type â†’ Mitigation Mapping

| Bias Type | Where It Originates | Mitigation Strategy |
|-----------|---------------------|---------------------|
| Historical | Data reflects past discrimination | Causal methods, reweighting, fairness constraints |
| Representation | Underrepresented groups in data | Resampling, data augmentation, collect more data |
| Measurement | Labels systematically wrong by group | Improve labeling, use multiple raters, audit labels |
| Aggregation | One model for all groups | Group-specific models, fairness constraints |
| Evaluation | Aggregate metrics hide disparity | Sliced evaluation, intersectional analysis |
| Deployment | Access, rollout disparity | Staged rollout, equity in access |
| Selection | Non-random data selection | Correct for selection bias, stratified sampling |
| Feedback loop | Model affects future data | Diversity injection, exploration, debiased logging |

---

## ğŸ”— Glossary

| Term | Definition |
|------|------------|
| **Protected attribute** | Characteristic (race, gender, etc.) that should not drive discriminatory decisions |
| **Disparate impact** | When a policy adversely affects a protected group, regardless of intent |
| **Disparate treatment** | Intentional discrimination based on protected attribute |
| **Proxy** | A feature that correlates with a protected attribute and can enable discrimination |
| **Base rate** | The rate of positive outcomes (Y=1) in a group |
| **Sliced evaluation** | Evaluating metrics separately for each subgroup |
| **Intersectionality** | Considering combinations of protected attributes (e.g., Black women) |
| **Feedback loop** | When model outputs influence future inputs, potentially amplifying bias |

---

---

## ğŸ“‘ Intervention Cheat Sheet

| If you observe... | Consider... |
|-------------------|-------------|
| Lower positive rate for protected group | Pre-processing: reweight/resample. Post-processing: lower threshold for protected group. |
| Higher FPR for protected group | Equalized odds constraint; threshold adjustment; investigate measurement/label bias. |
| Lower accuracy for minority slice | Representation biasâ€”resample, collect more data, or use group-specific components. |
| Proxy feature (e.g., zip) has high importance | Remove or downweight proxy; use adversarial debiasing; test post-removal. |
| Feedback loop (recs/suggestions) | Diversity injection; exploration; debiased logging. |
| No demographic data for evaluation | Optional disclosure; proxy-based analysis; external audit; outcome monitoring. |
| Conflicting fairness metrics | Document trade-off; get stakeholder alignment; choose based on domain. |

---

## Related Topics

| Topic | Link | Connection |
|-------|------|------------|
| Data Management | [02-data-management](../../phase-2-core-components/02-data-management) | Bias often originates in data collection and quality |
| Experiment Tracking | [04-model-training/03-experiment-tracking](../../phase-2-core-components/04-model-training/03-experiment-tracking.md) | Track fairness metrics across experiments |
| Model Monitoring | [06-monitoring-observability/01-model-monitoring](../../phase-3-operations-and-reliability/06-monitoring-observability/01-model-monitoring.md) | Extend monitoring to fairness metrics |
| Security & Privacy | [09-security-privacy](../../phase-3-operations-and-reliability/09-security-privacy) | Privacy-preserving fairness (e.g., federated learning) |
| Compliance | [09-security-privacy/04-compliance](../../phase-3-operations-and-reliability/09-security-privacy/04-compliance.md) | EU AI Act, NIST AI RMF, regulatory requirements |
| Recommendation Systems | [10-end-to-end-systems/01-recommendation-systems](../../phase-4-end-to-end-systems/10-end-to-end-systems/01-recommendation-systems.md) | Filter bubbles, feedback loops, diversity |
